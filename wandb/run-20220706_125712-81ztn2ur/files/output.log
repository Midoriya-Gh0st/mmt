
wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
1
CUDA: OK
Start loading the data....
  - Found cached train data
  - Found cached valid data
  - Found cached test data
<class 'src.dataset.Multimodal_Datasets'>
Finish loading the data....
### Note: You are running in unaligned mode.
n_train: 1284
dict: {'lr': 0.001, 'optim': 'Adam', 'num_epochs': 20, 'nlevels': 5, 'num_heads': 10, 'batch_size': 64, 'clip': 0.8, 'attn_dropout': 0.2, 'out_dropout': 0.1, 'embed_dropout': 0.2}
ON Test.
D:\mmt\modules\position_embedding.py:21: UserWarning: The number of elements in the out tensor of shape [50] is 50 which does not match the computed number of elements 375. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (375,). (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\cuda\RangeFactories.cu:251.)
  torch.arange(padding_idx + 1, max_pos, out=getattr(make_positions, buf_name))
D:\mmt\modules\position_embedding.py:21: UserWarning: The number of elements in the out tensor of shape [375] is 375 which does not match the computed number of elements 500. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (500,). (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\cuda\RangeFactories.cu:251.)
  torch.arange(padding_idx + 1, max_pos, out=getattr(make_positions, buf_name))
Traceback (most recent call last):
  File "D:\mmt\main.py", line 209, in <module>
    test_loss = train.initiate(hyp_params, train_loader, valid_loader, test_loader)
  File "D:\mmt\src\train.py", line 67, in initiate
    return train_model(settings, hyp_params, train_loader, valid_loader, test_loader)
  File "D:\mmt\src\train.py", line 260, in train_model
    _, results, truths = evaluate(model, ctc_a2l_module, ctc_v2l_module, criterion, test=True)
  File "D:\mmt\src\train.py", line 220, in evaluate
    preds, _ = net(text, audio, vision)
  File "D:\Anaconda\envs\project\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Anaconda\envs\project\lib\site-packages\torch\nn\parallel\data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "D:\Anaconda\envs\project\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\mmt\src\models.py", line 113, in forward
    h_ls = self.trans_l_mem(h_ls)
  File "D:\Anaconda\envs\project\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\mmt\modules\transformer.py", line 65, in forward
    x += self.embed_positions(x_in.transpose(0, 1)[:, :, 0]).transpose(0, 1)   # Add positional embedding
  File "D:\Anaconda\envs\project\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\mmt\modules\position_embedding.py", line 76, in forward
    return self.weights[device].index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.